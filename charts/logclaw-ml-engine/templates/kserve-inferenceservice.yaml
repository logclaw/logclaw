{{- if .Capabilities.APIVersions.Has "serving.kserve.io/v1beta1" }}
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: {{ include "logclaw-ml-engine.inferenceServiceName" . }}
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "logclaw-ml-engine.labels" . | nindent 4 }}
    app.kubernetes.io/component: kserve-predictor
  annotations:
    # Enable Prometheus scraping of KServe's built-in metrics endpoint.
    serving.kserve.io/enable-prometheus-scraping: "true"
    # LogClaw SLO declaration — consumed by alerting rules in prometheusrule.yaml.
    logclaw.io/slo: "sub-200ms-p99"
    logclaw.io/tenant: {{ include "logclaw-ml-engine.tenantId" . }}
spec:
  predictor:
    serviceAccountName: {{ include "logclaw-ml-engine.serviceAccountName" . }}

    # KServe native autoscaling — Knative-based, takes precedence over any
    # external HPA.  hpa-kserve.yaml provides a fallback HPA for clusters
    # without Knative/KPA installed.
    minReplicas: {{ .Values.mlEngine.kserve.minReplicas }}
    maxReplicas: {{ .Values.mlEngine.kserve.maxReplicas }}
    scaleTarget: {{ .Values.mlEngine.kserve.scaleTarget }}
    scaleMetric: concurrency

    torchserve:
      storageUri: "{{ .Values.global.objectStorage.provider }}://{{ .Values.global.objectStorage.bucket }}/models/anomaly/{{ .Values.mlEngine.kserve.modelVersion }}"
      runtimeVersion: {{ .Values.mlEngine.kserve.torchServeVersion | quote }}
      protocolVersion: grpc-v2

      resources:
        requests:
          cpu: {{ .Values.mlEngine.kserve.resources.requests.cpu | quote }}
          memory: {{ .Values.mlEngine.kserve.resources.requests.memory | quote }}
        limits:
          cpu: {{ .Values.mlEngine.kserve.resources.limits.cpu | quote }}
          memory: {{ .Values.mlEngine.kserve.resources.limits.memory | quote }}
          {{- if .Values.mlEngine.kserve.gpu.enabled }}
          nvidia.com/gpu: "1"
          {{- end }}

      env:
        - name: TENANT_ID
          value: {{ include "logclaw-ml-engine.tenantId" . | quote }}
        - name: MODEL_VERSION
          value: {{ .Values.mlEngine.kserve.modelVersion | quote }}
        - name: FEAST_FEATURE_SERVER
          value: "http://{{ include "logclaw-ml-engine.feastFQDN" . }}:{{ .Values.mlEngine.feast.service.httpPort }}"
        # TorchServe config location — the ConfigMap is mounted by KServe
        # via the torchserve-config volume defined below.
        - name: TS_CONFIG_FILE
          value: /mnt/models/config/config.properties

      volumes:
        - name: torchserve-config
          configMap:
            name: {{ include "logclaw-ml-engine.fullname" . }}-torchserve-config

      volumeMounts:
        - name: torchserve-config
          mountPath: /mnt/models/config
          readOnly: true

    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: {{ .Values.global.topologyKey | default "topology.kubernetes.io/zone" }}
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            serving.kserve.io/inferenceservice: {{ include "logclaw-ml-engine.inferenceServiceName" . }}
{{- end }}
