apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "logclaw-ingestion.fullname" . }}-vector-config
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "logclaw-ingestion.labels" . | nindent 4 }}
data:
  vector.toml: |
    # =========================================================================
    # Vector Configuration — LogClaw Ingestion
    # Mode: {{ .Values.mode }}
    # Tenant: {{ include "logclaw.tenantId" . }}
    # Generated by Helm chart version {{ .Chart.Version }}
    # =========================================================================

    # ---------------------------------------------------------------------------
    # Global settings
    # ---------------------------------------------------------------------------
    [api]
    enabled = true
    address = "0.0.0.0:8686"

    # ---------------------------------------------------------------------------
    # SOURCES
    # ---------------------------------------------------------------------------
    {{- if and (eq .Values.mode "deployment") .Values.sources.http.enabled }}

    # HTTP receiver — accepts JSON log batches from enterprise routers via PrivateLink
    [sources.http_receiver]
    type = "http_server"
    address = "0.0.0.0:{{ .Values.sources.http.port }}"
    encoding = "json"
    # Forward the X-Tenant-ID header for validation
    headers = ["X-Tenant-ID"]
    {{- if .Values.sources.http.tls.enabled }}
    [sources.http_receiver.tls]
    enabled = true
    crt_file = "/etc/vector/tls/tls.crt"
    key_file = "/etc/vector/tls/tls.key"
    {{- end }}

    {{- end }}

    {{- if eq .Values.mode "daemonset" }}

    # Kubernetes log source — reads container log files from host /var/log
    [sources.kubernetes_logs]
    type = "kubernetes_logs"
    auto_partial_merge = true
    self_node_name = "${VECTOR_SELF_NODE_NAME}"

    # Syslog source (optional)
    {{- if .Values.sources.syslog.enabled }}
    [sources.syslog_in]
    type = "syslog"
    address = "0.0.0.0:{{ .Values.sources.syslog.port }}"
    mode = "tcp"
    {{- end }}

    {{- end }}

    # Internal metrics — Vector's own operational telemetry
    [sources.internal_metrics]
    type = "internal_metrics"

    # ---------------------------------------------------------------------------
    # TRANSFORMS
    # ---------------------------------------------------------------------------

    # Step 1: Parse and normalize incoming log structure
    [transforms.parse_logs]
    type = "remap"
    {{- if eq .Values.mode "deployment" }}
    inputs = ["http_receiver"]
    {{- else }}
    inputs = ["kubernetes_logs"{{ if .Values.sources.syslog.enabled }}, "syslog_in"{{ end }}]
    {{- end }}
    source = '''
      # Ensure message field exists
      if !exists(.message) {
        .message = encode_json(.)
      }

      # Normalize log level to uppercase
      if exists(.level) {
        .level = upcase(string!(.level))
      } else if exists(.severity) {
        .level = upcase(string!(.severity))
      } else {
        .level = "INFO"
      }

      # Parse JSON message body if it is a string
      if is_string(.message) {
        parsed, err = parse_json(.message)
        if err == null {
          . = merge!(., parsed)
        }
      }
    '''

    # Step 2: Metadata enrichment — add tenant context and ingest timestamp
    [transforms.add_metadata]
    type = "remap"
    inputs = ["parse_logs"]
    source = '''
      # Stamp the ingestion time in RFC3339 format (UTC)
      .ingest_timestamp = now()

      # Inject tenant ID from the environment (set by Helm/Kubernetes)
      .tenant_id = get_env_var!("TENANT_ID")

      # Node and pod context (populated from Downward API env vars)
      .logclaw.node_name   = get_env_var("VECTOR_SELF_NODE_NAME") ?? ""
      .logclaw.pod_name    = get_env_var("VECTOR_SELF_POD_NAME") ?? ""
      .logclaw.namespace   = get_env_var("VECTOR_SELF_POD_NAMESPACE") ?? ""
      .logclaw.chart_ver   = "{{ .Chart.Version }}"

      # Validate that tenant ID from header (if present) matches the expected tenant
      if exists(.tenant_id_header) {
        if .tenant_id_header != .tenant_id {
          .logclaw.tenant_mismatch = true
          .logclaw.tenant_id_header = .tenant_id_header
        }
        del(.tenant_id_header)
      }
    '''

    # Step 3: Drop-sampling — reduce cardinality per log level before Kafka
    # DEBUG events: 1% pass through
    [transforms.sample_debug]
    type = "sample"
    inputs = ["add_metadata"]
    rate = 100
    key_field = "level"
    exclude = """
      .level != "DEBUG"
    """

    # INFO events: 10% pass through
    [transforms.sample_info]
    type = "sample"
    inputs = ["add_metadata"]
    rate = 10
    key_field = "level"
    exclude = """
      .level != "INFO"
    """

    # WARN and ERROR events: 100% pass through (no sampling)
    [transforms.pass_warn_error]
    type = "filter"
    inputs = ["add_metadata"]
    condition = '.level == "WARN" || .level == "ERROR"'

    # Step 4: Route events to appropriate sinks
    [transforms.route_by_level]
    type = "remap"
    inputs = ["sample_debug", "sample_info", "pass_warn_error"]
    source = '''
      # Tag sampled events so consumers know the sampling ratio applied
      if .level == "DEBUG" {
        .logclaw.sample_rate = 0.01
      } else if .level == "INFO" {
        .logclaw.sample_rate = 0.10
      } else {
        .logclaw.sample_rate = 1.0
      }
    '''

    # ---------------------------------------------------------------------------
    # SINKS
    # ---------------------------------------------------------------------------

    # Primary sink: Kafka
    [sinks.kafka_out]
    type = "kafka"
    inputs = ["route_by_level"]
    bootstrap_servers = "${KAFKA_BROKERS}"
    topic = "${KAFKA_TOPIC}"
    compression = "{{ .Values.sink.kafka.compression }}"
    message_timeout_ms = 30000
    # Partition by tenant_id so consumers can filter efficiently
    key_field = "tenant_id"

    [sinks.kafka_out.encoding]
    codec = "{{ .Values.sink.kafka.encoding }}"

    [sinks.kafka_out.batch]
    max_events = {{ .Values.sink.kafka.batchSize }}

    [sinks.kafka_out.buffer]
    type = "memory"
    max_events = 500
    when_full = "block"

    # Prometheus metrics exporter — scraped by the platform monitoring stack
    [sinks.prometheus_exporter]
    type = "prometheus_exporter"
    inputs = ["internal_metrics"]
    address = "0.0.0.0:9598"
    default_namespace = "logclaw_ingestion"
